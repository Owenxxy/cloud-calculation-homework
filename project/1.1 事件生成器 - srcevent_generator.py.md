1.1 事件生成器 - src/event\_generator.py

\#!/usr/bin/env python3

"""

Event Generator for Knative Pipeline

"""

import time

import json

import random

import requests

from datetime import datetime

from concurrent.futures import ThreadPoolExecutor

import uuid

import argparse



class EventGenerator:

&nbsp;   def \_\_init\_\_(self, broker\_url="http://broker-ingress.knative-eventing.svc.cluster.local/default/default"):

&nbsp;       self.broker\_url = broker\_url

&nbsp;       self.event\_types = \["log.ingest", "log.error", "log.audit"]

&nbsp;       self.sources = \["web-server", "api-gateway", "database", "auth-service"]

&nbsp;       

&nbsp;   def generate\_log\_event(self):

&nbsp;       """生成模拟日志事件"""

&nbsp;       event\_id = str(uuid.uuid4())

&nbsp;       timestamp = datetime.utcnow().isoformat() + "Z"

&nbsp;       

&nbsp;       log\_formats = \[

&nbsp;           f'{timestamp} - {random.choice(self.sources)} - INFO - User login successful',

&nbsp;           f'{timestamp} - {random.choice(self.sources)} - ERROR - Database connection failed',

&nbsp;           f'{timestamp} - {random.choice(self.sources)} - WARN - High memory usage detected',

&nbsp;           f'{timestamp} - {random.choice(self.sources)} - DEBUG - Processing request ID: {event\_id}'

&nbsp;       ]

&nbsp;       

&nbsp;       event = {

&nbsp;           "specversion": "1.0",

&nbsp;           "type": random.choice(self.event\_types),

&nbsp;           "source": f"event-generator/{random.choice(self.sources)}",

&nbsp;           "id": event\_id,

&nbsp;           "time": timestamp,

&nbsp;           "datacontenttype": "application/json",

&nbsp;           "data": {

&nbsp;               "log\_message": random.choice(log\_formats),

&nbsp;               "severity": random.choice(\["INFO", "ERROR", "WARN", "DEBUG"]),

&nbsp;               "service": random.choice(self.sources),

&nbsp;               "timestamp": timestamp,

&nbsp;               "metadata": {

&nbsp;                   "region": random.choice(\["us-east", "us-west", "eu-central"]),

&nbsp;                   "environment": random.choice(\["production", "staging", "development"]),

&nbsp;                   "version": f"1.{random.randint(0, 5)}.{random.randint(0, 20)}"

&nbsp;               }

&nbsp;           }

&nbsp;       }

&nbsp;       return event

&nbsp;   

&nbsp;   def send\_event(self, event):

&nbsp;       """发送事件到Knative Broker"""

&nbsp;       headers = {

&nbsp;           "Ce-Id": event\["id"],

&nbsp;           "Ce-Specversion": event\["specversion"],

&nbsp;           "Ce-Type": event\["type"],

&nbsp;           "Ce-Source": event\["source"],

&nbsp;           "Content-Type": event\["datacontenttype"]

&nbsp;       }

&nbsp;       

&nbsp;       try:

&nbsp;           response = requests.post(

&nbsp;               self.broker\_url,

&nbsp;               headers=headers,

&nbsp;               json=event\["data"],

&nbsp;               timeout=5

&nbsp;           )

&nbsp;           return response.status\_code == 202

&nbsp;       except Exception as e:

&nbsp;           print(f"发送事件失败: {e}")

&nbsp;           return False

&nbsp;   

&nbsp;   def generate\_load(self, events\_per\_second=10, duration\_seconds=60):

&nbsp;       """生成测试负载"""

&nbsp;       print(f"生成负载: {events\_per\_second} 事件/秒, 持续 {duration\_seconds} 秒")

&nbsp;       

&nbsp;       start\_time = time.time()

&nbsp;       event\_count = 0

&nbsp;       

&nbsp;       with ThreadPoolExecutor(max\_workers=10) as executor:

&nbsp;           while time.time() - start\_time < duration\_seconds:

&nbsp;               batch\_start = time.time()

&nbsp;               

&nbsp;               futures = \[]

&nbsp;               for \_ in range(events\_per\_second):

&nbsp;                   event = self.generate\_log\_event()

&nbsp;                   future = executor.submit(self.send\_event, event)

&nbsp;                   futures.append(future)

&nbsp;               

&nbsp;               successful = sum(1 for f in futures if f.result())

&nbsp;               event\_count += len(futures)

&nbsp;               

&nbsp;               batch\_time = time.time() - batch\_start

&nbsp;               if batch\_time < 1:

&nbsp;                   time.sleep(1 - batch\_time)

&nbsp;               

&nbsp;               print(f"发送 {len(futures)} 事件, 成功 {successful}")

&nbsp;       

&nbsp;       print(f"总发送事件: {event\_count}")

&nbsp;       return event\_count



if \_\_name\_\_ == "\_\_main\_\_":

&nbsp;   parser = argparse.ArgumentParser(description="Knative Pipeline 事件生成器")

&nbsp;   parser.add\_argument("--rate", type=int, default=10, help="事件每秒")

&nbsp;   parser.add\_argument("--duration", type=int, default=30, help="持续时间(秒)")

&nbsp;   parser.add\_argument("--broker", type=str, 

&nbsp;                      default="http://localhost:8080",

&nbsp;                      help="Broker URL")

&nbsp;   

&nbsp;   args = parser.parse\_args()

&nbsp;   

&nbsp;   generator = EventGenerator(args.broker)

&nbsp;   generator.generate\_load(args.rate, args.duration)





1.2 数据摄取服务 - src/ingestor\_service.py

\#!/usr/bin/env python3

"""

Ingestor Service - 管道第一阶段

"""

from flask import Flask, request, jsonify

import json

import logging

from datetime import datetime

import os



app = Flask(\_\_name\_\_)

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(\_\_name\_\_)



class Ingestor:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.service\_name = "ingestor-service"

&nbsp;       self.version = os.getenv("VERSION", "1.0.0")

&nbsp;       

&nbsp;   def process\_event(self, event\_data, cloud\_event\_headers):

&nbsp;       """处理和丰富事件数据"""

&nbsp;       try:

&nbsp;           event\_id = cloud\_event\_headers.get('ce-id')

&nbsp;           event\_type = cloud\_event\_headers.get('ce-type')

&nbsp;           event\_source = cloud\_event\_headers.get('ce-source')

&nbsp;           

&nbsp;           if not event\_data or 'log\_message' not in event\_data:

&nbsp;               raise ValueError("无效的事件数据")

&nbsp;           

&nbsp;           enriched\_data = {

&nbsp;               \*\*event\_data,

&nbsp;               "ingestion\_metadata": {

&nbsp;                   "ingestor\_service": self.service\_name,

&nbsp;                   "ingestion\_time": datetime.utcnow().isoformat() + "Z",

&nbsp;                   "ingestion\_id": event\_id,

&nbsp;                   "pipeline\_stage": "ingestion"

&nbsp;               },

&nbsp;               "validation": {

&nbsp;                   "is\_valid": True,

&nbsp;                   "checks\_passed": \["format\_check", "timestamp\_check"],

&nbsp;                   "schema\_version": "1.0"

&nbsp;               }

&nbsp;           }

&nbsp;           

&nbsp;           if 'timestamps' not in enriched\_data:

&nbsp;               enriched\_data\['timestamps'] = {}

&nbsp;           enriched\_data\['timestamps']\['ingested\_at'] = datetime.utcnow().isoformat() + "Z"

&nbsp;           

&nbsp;           logger.info(f"处理事件 {event\_id} 来自 {event\_source}")

&nbsp;           return enriched\_data

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"处理事件错误: {e}")

&nbsp;           raise



ingestor = Ingestor()



@app.route('/', methods=\['POST'])

def handle\_event():

&nbsp;   """处理传入的CloudEvents"""

&nbsp;   try:

&nbsp;       ce\_headers = {k.lower(): v for k, v in request.headers.items() 

&nbsp;                    if k.lower().startswith('ce-')}

&nbsp;       

&nbsp;       event\_data = request.get\_json()

&nbsp;       enriched\_data = ingestor.process\_event(event\_data, ce\_headers)

&nbsp;       

&nbsp;       return jsonify({

&nbsp;           "status": "success",

&nbsp;           "message": "事件摄取成功",

&nbsp;           "data": enriched\_data

&nbsp;       }), 200

&nbsp;       

&nbsp;   except Exception as e:

&nbsp;       logger.error(f"处理事件错误: {e}")

&nbsp;       return jsonify({

&nbsp;           "status": "error",

&nbsp;           "message": str(e)

&nbsp;       }), 400



@app.route('/health', methods=\['GET'])

def health\_check():

&nbsp;   """健康检查端点"""

&nbsp;   return jsonify({

&nbsp;       "status": "healthy",

&nbsp;       "service": "ingestor-service",

&nbsp;       "version": ingestor.version,

&nbsp;       "timestamp": datetime.utcnow().isoformat() + "Z"

&nbsp;   }), 200



if \_\_name\_\_ == '\_\_main\_\_':

&nbsp;   port = int(os.getenv('PORT', 8080))

&nbsp;   app.run(host='0.0.0.0', port=port, debug=False)



1.3 数据解析服务 - src/parser\_service.py

\#!/usr/bin/env python3

"""

Parser Service - 管道第二阶段

"""

from flask import Flask, request, jsonify

import re

import json

import logging

from datetime import datetime

import os



app = Flask(\_\_name\_\_)

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(\_\_name\_\_)



class LogParser:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.patterns = {

&nbsp;           "nginx": re.compile(r'(?P<ip>\\S+) - - \\\[(?P<timestamp>.\*?)\\] "(?P<method>\\S+) (?P<path>\\S+) HTTP/\\d\\.\\d" (?P<status>\\d+) (?P<size>\\d+)'),

&nbsp;           "syslog": re.compile(r'(?P<timestamp>\\w+\\s+\\d+\\s+\\d+:\\d+:\\d+) (?P<host>\\S+) (?P<process>\\S+): (?P<message>.\*)'),

&nbsp;           "json": re.compile(r'^{.\*}$'),

&nbsp;       }

&nbsp;       

&nbsp;   def parse\_log\_message(self, log\_message):

&nbsp;       """解析日志消息为结构化格式"""

&nbsp;       parsed\_data = {

&nbsp;           "original\_message": log\_message,

&nbsp;           "parsed\_components": {},

&nbsp;           "parser\_used": "unknown"

&nbsp;       }

&nbsp;       

&nbsp;       for parser\_name, pattern in self.patterns.items():

&nbsp;           if parser\_name == "json" and self.patterns\["json"].match(log\_message):

&nbsp;               try:

&nbsp;                   parsed\_data\["parsed\_components"] = json.loads(log\_message)

&nbsp;                   parsed\_data\["parser\_used"] = "json"

&nbsp;                   return parsed\_data

&nbsp;               except json.JSONDecodeError:

&nbsp;                   continue

&nbsp;                   

&nbsp;           match = pattern.match(log\_message)

&nbsp;           if match:

&nbsp;               parsed\_data\["parsed\_components"] = match.groupdict()

&nbsp;               parsed\_data\["parser\_used"] = parser\_name

&nbsp;               break

&nbsp;       

&nbsp;       if parsed\_data\["parser\_used"] == "unknown":

&nbsp;           parts = log\_message.split(" - ")

&nbsp;           if len(parts) >= 3:

&nbsp;               parsed\_data\["parsed\_components"] = {

&nbsp;                   "timestamp": parts\[0],

&nbsp;                   "source": parts\[1],

&nbsp;                   "level": parts\[2],

&nbsp;                   "message": " - ".join(parts\[3:]) if len(parts) > 3 else ""

&nbsp;               }

&nbsp;               parsed\_data\["parser\_used"] = "simple\_split"

&nbsp;       

&nbsp;       return parsed\_data



class ParserService:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.service\_name = "parser-service"

&nbsp;       self.parser = LogParser()

&nbsp;       

&nbsp;   def process(self, event\_data):

&nbsp;       """处理事件数据"""

&nbsp;       try:

&nbsp;           if 'log\_message' not in event\_data:

&nbsp;               raise ValueError("事件数据中没有log\_message")

&nbsp;           

&nbsp;           parsed\_log = self.parser.parse\_log\_message(event\_data\['log\_message'])

&nbsp;           

&nbsp;           enriched\_data = {

&nbsp;               \*\*event\_data,

&nbsp;               "parsed\_data": parsed\_log,

&nbsp;               "processing\_metadata": {

&nbsp;                   "parser\_service": self.service\_name,

&nbsp;                   "parsed\_at": datetime.utcnow().isoformat() + "Z",

&nbsp;                   "parser\_version": "1.0"

&nbsp;               }

&nbsp;           }

&nbsp;           

&nbsp;           if 'timestamps' in enriched\_data:

&nbsp;               enriched\_data\['timestamps']\['parsed\_at'] = datetime.utcnow().isoformat() + "Z"

&nbsp;           

&nbsp;           logger.info(f"解析日志使用 {parsed\_log\['parser\_used']} 解析器")

&nbsp;           return enriched\_data

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"解析事件错误: {e}")

&nbsp;           raise



parser\_service = ParserService()



@app.route('/', methods=\['POST'])

def handle\_event():

&nbsp;   """处理来自摄取器的事件"""

&nbsp;   try:

&nbsp;       event\_data = request.get\_json()

&nbsp;       

&nbsp;       if isinstance(event\_data, dict) and 'data' in event\_data:

&nbsp;           event\_data = event\_data\['data']

&nbsp;       

&nbsp;       parsed\_data = parser\_service.process(event\_data)

&nbsp;       

&nbsp;       return jsonify({

&nbsp;           "status": "success",

&nbsp;           "message": "日志解析成功",

&nbsp;           "data": parsed\_data

&nbsp;       }), 200

&nbsp;       

&nbsp;   except Exception as e:

&nbsp;       logger.error(f"解析器错误: {e}")

&nbsp;       return jsonify({

&nbsp;           "status": "error",

&nbsp;           "message": str(e)

&nbsp;       }), 400



@app.route('/health', methods=\['GET'])

def health\_check():

&nbsp;   """健康检查"""

&nbsp;   return jsonify({

&nbsp;       "status": "healthy",

&nbsp;       "service": "parser-service",

&nbsp;       "patterns\_loaded": len(parser\_service.parser.patterns)

&nbsp;   }), 200



if \_\_name\_\_ == '\_\_main\_\_':

&nbsp;   port = int(os.getenv('PORT', 8080))

&nbsp;   app.run(host='0.0.0.0', port=port, debug=False)





1.4 数据转换服务 - src/transformer\_service.py

\#!/usr/bin/env python3

"""

Transformer Service - 管道第三阶段

"""

from flask import Flask, request, jsonify

import hashlib

import json

import logging

from datetime import datetime

import os

import re



app = Flask(\_\_name\_\_)

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(\_\_name\_\_)



class DataTransformer:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.transformations = {

&nbsp;           "mask\_pii": self.mask\_personal\_info,

&nbsp;           "enrich\_geo": self.enrich\_geolocation,

&nbsp;           "calculate\_hash": self.calculate\_event\_hash,

&nbsp;           "normalize\_fields": self.normalize\_field\_names

&nbsp;       }

&nbsp;       

&nbsp;   def mask\_personal\_info(self, data):

&nbsp;       """掩码个人身份信息"""

&nbsp;       if 'parsed\_data' in data and 'parsed\_components' in data\['parsed\_data']:

&nbsp;           components = data\['parsed\_data']\['parsed\_components']

&nbsp;           if 'message' in components:

&nbsp;               components\['message'] = re.sub(

&nbsp;                   r'\\b\[A-Za-z0-9.\_%+-]+@\[A-Za-z0-9.-]+\\.\[A-Z|a-z]{2,}\\b',

&nbsp;                   '\[EMAIL\_REDACTED]',

&nbsp;                   components\['message']

&nbsp;               )

&nbsp;           if 'ip' in components:

&nbsp;               parts = components\['ip'].split('.')

&nbsp;               if len(parts) == 4:

&nbsp;                   components\['ip'] = f"{parts\[0]}.\[REDACTED]"

&nbsp;       return data

&nbsp;   

&nbsp;   def enrich\_geolocation(self, data):

&nbsp;       """基于IP添加地理位置数据"""

&nbsp;       if ('parsed\_data' in data and 

&nbsp;           'parsed\_components' in data\['parsed\_data'] and 

&nbsp;           'ip' in data\['parsed\_data']\['parsed\_components']):

&nbsp;           

&nbsp;           ip = data\['parsed\_data']\['parsed\_components']\['ip']

&nbsp;           if ip.startswith('192.168.'):

&nbsp;               geo = {"country": "Internal", "city": "LAN"}

&nbsp;           elif ip.startswith('10.'):

&nbsp;               geo = {"country": "Private", "city": "Network"}

&nbsp;           else:

&nbsp;               ip\_hash = int(hashlib.md5(ip.encode()).hexdigest(), 16)

&nbsp;               countries = \["USA", "Germany", "Japan", "UK", "Australia"]

&nbsp;               cities = \["New York", "Berlin", "Tokyo", "London", "Sydney"]

&nbsp;               

&nbsp;               idx = ip\_hash % len(countries)

&nbsp;               geo = {

&nbsp;                   "country": countries\[idx],

&nbsp;                   "city": cities\[idx],

&nbsp;                   "coordinates": {

&nbsp;                       "lat": 40 + (ip\_hash % 20) - 10,

&nbsp;                       "lon": -70 + (ip\_hash % 40) - 20

&nbsp;                   }

&nbsp;               }

&nbsp;           

&nbsp;           data\['geolocation'] = geo

&nbsp;       

&nbsp;       return data

&nbsp;   

&nbsp;   def calculate\_event\_hash(self, data):

&nbsp;       """计算事件哈希值用于去重"""

&nbsp;       event\_str = json.dumps(data, sort\_keys=True)

&nbsp;       data\['event\_hash'] = hashlib.sha256(event\_str.encode()).hexdigest()

&nbsp;       return data

&nbsp;   

&nbsp;   def normalize\_field\_names(self, data):

&nbsp;       """规范化字段名称"""

&nbsp;       field\_mappings = {

&nbsp;           'timestamp': 'event\_timestamp',

&nbsp;           'time': 'event\_time',

&nbsp;           'msg': 'message',

&nbsp;           'lvl': 'level'

&nbsp;       }

&nbsp;       

&nbsp;       if 'parsed\_data' in data and 'parsed\_components' in data\['parsed\_data']:

&nbsp;           components = data\['parsed\_data']\['parsed\_components']

&nbsp;           for old, new in field\_mappings.items():

&nbsp;               if old in components:

&nbsp;                   components\[new] = components.pop(old)

&nbsp;       

&nbsp;       return data

&nbsp;   

&nbsp;   def apply\_transformations(self, data, transformations=None):

&nbsp;       """应用转换"""

&nbsp;       if transformations is None:

&nbsp;           transformations = self.transformations.keys()

&nbsp;       

&nbsp;       transformed\_data = data.copy()

&nbsp;       

&nbsp;       for transform\_name in transformations:

&nbsp;           if transform\_name in self.transformations:

&nbsp;               logger.info(f"应用转换: {transform\_name}")

&nbsp;               transformed\_data = self.transformations\[transform\_name](transformed\_data)

&nbsp;       

&nbsp;       return transformed\_data



class TransformerService:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.service\_name = "transformer-service"

&nbsp;       self.transformer = DataTransformer()

&nbsp;       

&nbsp;   def process(self, event\_data):

&nbsp;       """处理事件转换"""

&nbsp;       try:

&nbsp;           if isinstance(event\_data, dict) and 'data' in event\_data:

&nbsp;               event\_data = event\_data\['data']

&nbsp;           

&nbsp;           transformed\_data = self.transformer.apply\_transformations(

&nbsp;               event\_data,

&nbsp;               \["mask\_pii", "enrich\_geo", "calculate\_hash", "normalize\_fields"]

&nbsp;           )

&nbsp;           

&nbsp;           transformed\_data\['transformation\_metadata'] = {

&nbsp;               "transformer\_service": self.service\_name,

&nbsp;               "transformed\_at": datetime.utcnow().isoformat() + "Z",

&nbsp;               "transformations\_applied": list(self.transformer.transformations.keys())

&nbsp;           }

&nbsp;           

&nbsp;           if 'timestamps' in transformed\_data:

&nbsp;               transformed\_data\['timestamps']\['transformed\_at'] = datetime.utcnow().isoformat() + "Z"

&nbsp;           

&nbsp;           logger.info(f"转换事件, 哈希: {transformed\_data.get('event\_hash', 'N/A')\[:8]}")

&nbsp;           return transformed\_data

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"转换事件错误: {e}")

&nbsp;           raise



transformer\_service = TransformerService()



@app.route('/', methods=\['POST'])

def handle\_event():

&nbsp;   """处理来自解析器的事件"""

&nbsp;   try:

&nbsp;       event\_data = request.get\_json()

&nbsp;       transformed\_data = transformer\_service.process(event\_data)

&nbsp;       

&nbsp;       return jsonify({

&nbsp;           "status": "success",

&nbsp;           "message": "事件转换成功",

&nbsp;           "data": transformed\_data

&nbsp;       }), 200

&nbsp;       

&nbsp;   except Exception as e:

&nbsp;       logger.error(f"转换器错误: {e}")

&nbsp;       return jsonify({

&nbsp;           "status": "error",

&nbsp;           "message": str(e)

&nbsp;       }), 400



@app.route('/health', methods=\['GET'])

def health\_check():

&nbsp;   """健康检查"""

&nbsp;   return jsonify({

&nbsp;       "status": "healthy",

&nbsp;       "service": "transformer-service",

&nbsp;       "transformations\_available": list(transformer\_service.transformer.transformations.keys())

&nbsp;   }), 200



if \_\_name\_\_ == '\_\_main\_\_':

&nbsp;   port = int(os.getenv('PORT', 8080))

&nbsp;   app.run(host='0.0.0.0', port=port, debug=False)





1.5 数据加载服务 - src/loader\_service.py

\#!/usr/bin/env python3

"""

Loader Service - 管道最后阶段

"""

from flask import Flask, request, jsonify

import json

import logging

from datetime import datetime

import os

import sqlite3

import csv



app = Flask(\_\_name\_\_)

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(\_\_name\_\_)



class DataLoader:

&nbsp;   def \_\_init\_\_(self, storage\_path="./data"):

&nbsp;       self.storage\_path = storage\_path

&nbsp;       os.makedirs(storage\_path, exist\_ok=True)

&nbsp;       

&nbsp;       self.db\_path = os.path.join(storage\_path, "events.db")

&nbsp;       self.init\_database()

&nbsp;       

&nbsp;   def init\_database(self):

&nbsp;       """初始化SQLite数据库"""

&nbsp;       conn = sqlite3.connect(self.db\_path)

&nbsp;       cursor = conn.cursor()

&nbsp;       

&nbsp;       cursor.execute('''

&nbsp;           CREATE TABLE IF NOT EXISTS processed\_events (

&nbsp;               id TEXT PRIMARY KEY,

&nbsp;               event\_hash TEXT UNIQUE,

&nbsp;               service TEXT,

&nbsp;               severity TEXT,

&nbsp;               timestamp TEXT,

&nbsp;               parsed\_components TEXT,

&nbsp;               geolocation TEXT,

&nbsp;               processed\_at TEXT,

&nbsp;               storage\_format TEXT

&nbsp;           )

&nbsp;       ''')

&nbsp;       

&nbsp;       conn.commit()

&nbsp;       conn.close()

&nbsp;       

&nbsp;   def store\_to\_sqlite(self, event\_data):

&nbsp;       """存储到SQLite数据库"""

&nbsp;       try:

&nbsp;           conn = sqlite3.connect(self.db\_path)

&nbsp;           cursor = conn.cursor()

&nbsp;           

&nbsp;           event\_id = event\_data.get('ingestion\_metadata', {}).get('ingestion\_id', '')

&nbsp;           event\_hash = event\_data.get('event\_hash', '')

&nbsp;           service = event\_data.get('service', 'unknown')

&nbsp;           severity = event\_data.get('severity', 'INFO')

&nbsp;           timestamp = event\_data.get('timestamp', '')

&nbsp;           

&nbsp;           parsed\_json = json.dumps(

&nbsp;               event\_data.get('parsed\_data', {}).get('parsed\_components', {})

&nbsp;           )

&nbsp;           

&nbsp;           geo\_json = json.dumps(event\_data.get('geolocation', {}))

&nbsp;           

&nbsp;           cursor.execute('''

&nbsp;               INSERT OR IGNORE INTO processed\_events 

&nbsp;               (id, event\_hash, service, severity, timestamp, parsed\_components, 

&nbsp;                geolocation, processed\_at, storage\_format)

&nbsp;               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)

&nbsp;           ''', (

&nbsp;               event\_id,

&nbsp;               event\_hash,

&nbsp;               service,

&nbsp;               severity,

&nbsp;               timestamp,

&nbsp;               parsed\_json,

&nbsp;               geo\_json,

&nbsp;               datetime.utcnow().isoformat() + "Z",

&nbsp;               'json'

&nbsp;           ))

&nbsp;           

&nbsp;           conn.commit()

&nbsp;           conn.close()

&nbsp;           return True

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"存储到SQLite错误: {e}")

&nbsp;           return False

&nbsp;   

&nbsp;   def store\_to\_json\_file(self, event\_data):

&nbsp;       """存储到JSON文件"""

&nbsp;       try:

&nbsp;           timestamp = datetime.utcnow().strftime("%Y%m%d")

&nbsp;           json\_dir = os.path.join(self.storage\_path, "json", timestamp)

&nbsp;           os.makedirs(json\_dir, exist\_ok=True)

&nbsp;           

&nbsp;           filename = os.path.join(json\_dir, f"event\_{event\_data.get('event\_hash', '')\[:8]}.json")

&nbsp;           

&nbsp;           with open(filename, 'w') as f:

&nbsp;               json.dump(event\_data, f, indent=2)

&nbsp;           

&nbsp;           return True

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"存储到JSON文件错误: {e}")

&nbsp;           return False

&nbsp;   

&nbsp;   def store\_to\_csv(self, event\_data):

&nbsp;       """存储到CSV文件"""

&nbsp;       try:

&nbsp;           timestamp = datetime.utcnow().strftime("%Y%m%d")

&nbsp;           csv\_dir = os.path.join(self.storage\_path, "csv", timestamp)

&nbsp;           os.makedirs(csv\_dir, exist\_ok=True)

&nbsp;           

&nbsp;           csv\_file = os.path.join(csv\_dir, "events.csv")

&nbsp;           file\_exists = os.path.isfile(csv\_file)

&nbsp;           

&nbsp;           row = {

&nbsp;               'event\_id': event\_data.get('ingestion\_metadata', {}).get('ingestion\_id', ''),

&nbsp;               'event\_hash': event\_data.get('event\_hash', ''),

&nbsp;               'service': event\_data.get('service', ''),

&nbsp;               'severity': event\_data.get('severity', ''),

&nbsp;               'timestamp': event\_data.get('timestamp', ''),

&nbsp;               'processed\_at': datetime.utcnow().isoformat() + "Z"

&nbsp;           }

&nbsp;           

&nbsp;           with open(csv\_file, 'a', newline='') as f:

&nbsp;               writer = csv.DictWriter(f, fieldnames=row.keys())

&nbsp;               if not file\_exists:

&nbsp;                   writer.writeheader()

&nbsp;               writer.writerow(row)

&nbsp;           

&nbsp;           return True

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"存储到CSV错误: {e}")

&nbsp;           return False

&nbsp;   

&nbsp;   def store\_to\_console(self, event\_data):

&nbsp;       """日志输出到控制台"""

&nbsp;       try:

&nbsp;           summary = {

&nbsp;               'id': event\_data.get('ingestion\_metadata', {}).get('ingestion\_id', '')\[:8],

&nbsp;               'service': event\_data.get('service', ''),

&nbsp;               'severity': event\_data.get('severity', ''),

&nbsp;               'timestamp': event\_data.get('timestamp', ''),

&nbsp;               'hash': event\_data.get('event\_hash', '')\[:8]

&nbsp;           }

&nbsp;           logger.info(f"事件存储: {summary}")

&nbsp;           return True

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"控制台日志错误: {e}")

&nbsp;           return False

&nbsp;   

&nbsp;   def store\_event(self, event\_data):

&nbsp;       """存储事件到所有配置的存储后端"""

&nbsp;       results = {

&nbsp;           'sqlite': self.store\_to\_sqlite(event\_data),

&nbsp;           'json\_file': self.store\_to\_json\_file(event\_data),

&nbsp;           'csv': self.store\_to\_csv(event\_data),

&nbsp;           'console': self.store\_to\_console(event\_data)

&nbsp;       }

&nbsp;       

&nbsp;       return results



class LoaderService:

&nbsp;   def \_\_init\_\_(self):

&nbsp;       self.service\_name = "loader-service"

&nbsp;       self.loader = DataLoader()

&nbsp;       self.processed\_count = 0

&nbsp;       

&nbsp;   def process(self, event\_data):

&nbsp;       """处理和存储事件"""

&nbsp;       try:

&nbsp;           if isinstance(event\_data, dict) and 'data' in event\_data:

&nbsp;               event\_data = event\_data\['data']

&nbsp;           

&nbsp;           event\_data\['loading\_metadata'] = {

&nbsp;               "loader\_service": self.service\_name,

&nbsp;               "loaded\_at": datetime.utcnow().isoformat() + "Z",

&nbsp;               "storage\_backends": \["sqlite", "json\_file", "csv", "console"]

&nbsp;           }

&nbsp;           

&nbsp;           storage\_results = self.loader.store\_event(event\_data)

&nbsp;           

&nbsp;           if 'timestamps' in event\_data:

&nbsp;               event\_data\['timestamps']\['loaded\_at'] = datetime.utcnow().isoformat() + "Z"

&nbsp;           

&nbsp;           self.processed\_count += 1

&nbsp;           

&nbsp;           response = {

&nbsp;               "status": "success",

&nbsp;               "message": "事件加载成功",

&nbsp;               "data": event\_data,

&nbsp;               "storage\_results": storage\_results,

&nbsp;               "metrics": {

&nbsp;                   "total\_processed": self.processed\_count,

&nbsp;                   "success\_rate": sum(storage\_results.values()) / len(storage\_results)

&nbsp;               }

&nbsp;           }

&nbsp;           

&nbsp;           logger.info(f"加载事件 {self.processed\_count}, 结果: {storage\_results}")

&nbsp;           return response

&nbsp;           

&nbsp;       except Exception as e:

&nbsp;           logger.error(f"加载事件错误: {e}")

&nbsp;           raise



loader\_service = LoaderService()



@app.route('/', methods=\['POST'])

def handle\_event():

&nbsp;   """处理来自转换器的事件"""

&nbsp;   try:

&nbsp;       event\_data = request.get\_json()

&nbsp;       result = loader\_service.process(event\_data)

&nbsp;       

&nbsp;       return jsonify(result), 200

&nbsp;       

&nbsp;   except Exception as e:

&nbsp;       logger.error(f"加载器错误: {e}")

&nbsp;       return jsonify({

&nbsp;           "status": "error",

&nbsp;           "message": str(e)

&nbsp;       }), 400



@app.route('/health', methods=\['GET'])

def health\_check():

&nbsp;   """健康检查"""

&nbsp;   conn = sqlite3.connect(loader\_service.loader.db\_path)

&nbsp;   cursor = conn.cursor()

&nbsp;   cursor.execute("SELECT COUNT(\*) FROM processed\_events")

&nbsp;   count = cursor.fetchone()\[0]

&nbsp;   conn.close()

&nbsp;   

&nbsp;   return jsonify({

&nbsp;       "status": "healthy",

&nbsp;       "service": "loader-service",

&nbsp;       "events\_stored": count,

&nbsp;       "total\_processed": loader\_service.processed\_count

&nbsp;   }), 200



@app.route('/metrics', methods=\['GET'])

def get\_metrics():

&nbsp;   """获取管道指标"""

&nbsp;   conn = sqlite3.connect(loader\_service.loader.db\_path)

&nbsp;   cursor = conn.cursor()

&nbsp;   

&nbsp;   cursor.execute("SELECT COUNT(\*) FROM processed\_events")

&nbsp;   total\_events = cursor.fetchone()\[0]

&nbsp;   

&nbsp;   cursor.execute("SELECT severity, COUNT(\*) as count FROM processed\_events GROUP BY severity")

&nbsp;   severity\_counts = dict(cursor.fetchall())

&nbsp;   

&nbsp;   conn.close()

&nbsp;   

&nbsp;   return jsonify({

&nbsp;       "total\_events": total\_events,

&nbsp;       "severity\_distribution": severity\_counts,

&nbsp;       "loader\_processed": loader\_service.processed\_count,

&nbsp;       "service\_status": "active"

&nbsp;   }), 200



if \_\_name\_\_ == '\_\_main\_\_':

&nbsp;   port = int(os.getenv('PORT', 8080))

&nbsp;   app.run(host='0.0.0.0', port=port, debug=False)





2\. Dockerfile和依赖文件

2.1 requirements.txt

txt

Flask==2.3.3

requests==2.31.0

gunicorn==21.2.0

prometheus-client==0.18.0

2.2 Dockerfile (通用)

dockerfile

FROM python:3.9-slim



WORKDIR /app



COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt



\# 这里需要根据具体服务复制不同的文件

\# COPY app.py .



RUN useradd -m -u 1000 appuser \&\& chown -R appuser:appuser /app

USER appuser



EXPOSE 8080



CMD \["python", "app.py"]

3\. Kubernetes配置文件

3.1 kubernetes/namespace.yaml

yaml

apiVersion: v1

kind: Namespace

metadata:

&nbsp; name: serverless-pipeline

&nbsp; labels:

&nbsp;   name: serverless-pipeline

&nbsp;   environment: development

3.2 kubernetes/services.yaml

yaml

\# Ingestor Service

apiVersion: serving.knative.dev/v1

kind: Service

metadata:

&nbsp; name: ingestor-service

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; template:

&nbsp;   metadata:

&nbsp;     annotations:

&nbsp;       autoscaling.knative.dev/target: "10"

&nbsp;       autoscaling.knative.dev/minScale: "0"

&nbsp;       autoscaling.knative.dev/maxScale: "10"

&nbsp;   spec:

&nbsp;     containers:

&nbsp;       - image: localhost:5000/ingestor-service:latest

&nbsp;         ports:

&nbsp;           - containerPort: 8080

&nbsp;         env:

&nbsp;           - name: VERSION

&nbsp;             value: "1.0.0"

&nbsp;         readinessProbe:

&nbsp;           httpGet:

&nbsp;             path: /health

&nbsp;             port: 8080

&nbsp;           initialDelaySeconds: 5

&nbsp;           periodSeconds: 10

---

\# Parser Service

apiVersion: serving.knative.dev/v1

kind: Service

metadata:

&nbsp; name: parser-service

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; template:

&nbsp;   metadata:

&nbsp;     annotations:

&nbsp;       autoscaling.knative.dev/target: "10"

&nbsp;       autoscaling.knative.dev/minScale: "0"

&nbsp;       autoscaling.knative.dev/maxScale: "10"

&nbsp;   spec:

&nbsp;     containers:

&nbsp;       - image: localhost:5000/parser-service:latest

&nbsp;         ports:

&nbsp;           - containerPort: 8080

---

\# Transformer Service

apiVersion: serving.knative.dev/v1

kind: Service

metadata:

&nbsp; name: transformer-service

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; template:

&nbsp;   metadata:

&nbsp;     annotations:

&nbsp;       autoscaling.knative.dev/target: "10"

&nbsp;       autoscaling.knative.dev/minScale: "0"

&nbsp;       autoscaling.knative.dev/maxScale: "10"

&nbsp;   spec:

&nbsp;     containers:

&nbsp;       - image: localhost:5000/transformer-service:latest

&nbsp;         ports:

&nbsp;           - containerPort: 8080

---

\# Loader Service

apiVersion: serving.knative.dev/v1

kind: Service

metadata:

&nbsp; name: loader-service

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; template:

&nbsp;   metadata:

&nbsp;     annotations:

&nbsp;       autoscaling.knative.dev/target: "10"

&nbsp;       autoscaling.knative.dev/minScale: "0"

&nbsp;       autoscaling.knative.dev/maxScale: "10"

&nbsp;   spec:

&nbsp;     containers:

&nbsp;       - image: localhost:5000/loader-service:latest

&nbsp;         ports:

&nbsp;           - containerPort: 8080

&nbsp;         volumeMounts:

&nbsp;           - name: data-volume

&nbsp;             mountPath: /data

&nbsp;     volumes:

&nbsp;       - name: data-volume

&nbsp;         emptyDir: {}

3.3 kubernetes/events.yaml

yaml

\# Broker

apiVersion: eventing.knative.dev/v1

kind: Broker

metadata:

&nbsp; name: default

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; config:

&nbsp;   apiVersion: v1

&nbsp;   kind: ConfigMap

&nbsp;   name: config-br-default-channel

&nbsp;   namespace: knative-eventing



\# Triggers

---

apiVersion: eventing.knative.dev/v1

kind: Trigger

metadata:

&nbsp; name: ingestor-trigger

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; broker: default

&nbsp; filter:

&nbsp;   attributes:

&nbsp;     type: "log.ingest"

&nbsp; subscriber:

&nbsp;   ref:

&nbsp;     apiVersion: serving.knative.dev/v1

&nbsp;     kind: Service

&nbsp;     name: ingestor-service

---

apiVersion: eventing.knative.dev/v1

kind: Trigger

metadata:

&nbsp; name: parser-trigger

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; broker: default

&nbsp; filter:

&nbsp;   attributes:

&nbsp;     source: "ingestor-service"

&nbsp; subscriber:

&nbsp;   ref:

&nbsp;     apiVersion: serving.knative.dev/v1

&nbsp;     kind: Service

&nbsp;     name: parser-service

---

apiVersion: eventing.knative.dev/v1

kind: Trigger

metadata:

&nbsp; name: transformer-trigger

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; broker: default

&nbsp; filter:

&nbsp;   attributes:

&nbsp;     source: "parser-service"

&nbsp; subscriber:

&nbsp;   ref:

&nbsp;     apiVersion: serving.knative.dev/v1

&nbsp;     kind: Service

&nbsp;     name: transformer-service

---

apiVersion: eventing.knative.dev/v1

kind: Trigger

metadata:

&nbsp; name: loader-trigger

&nbsp; namespace: serverless-pipeline

spec:

&nbsp; broker: default

&nbsp; filter:

&nbsp;   attributes:

&nbsp;     source: "transformer-service"

&nbsp; subscriber:

&nbsp;   ref:

&nbsp;     apiVersion: serving.knative.dev/v1

&nbsp;     kind: Service

&nbsp;     name: loader-service

4\. 部署和测试脚本

4.1 scripts/deploy.sh

bash

\#!/bin/bash



\# 部署Knative Serverless Pipeline

set -e



echo "=== 部署Knative Serverless Pipeline ==="



\# 创建命名空间

echo "创建命名空间..."

kubectl apply -f kubernetes/namespace.yaml



\# 等待命名空间创建

sleep 2



\# 部署服务

echo "部署Knative服务..."

kubectl apply -f kubernetes/services.yaml



\# 部署事件配置

echo "部署事件配置..."

kubectl apply -f kubernetes/events.yaml



echo "=== 部署完成 ==="

echo ""

echo "检查部署状态:"

echo "kubectl get ksvc -n serverless-pipeline"

echo "kubectl get pods -n serverless-pipeline"

echo ""

echo "获取服务URL:"

echo "kubectl get ksvc ingestor-service -n serverless-pipeline -o jsonpath='{.status.url}'"

4.2 scripts/test.sh

bash

\#!/bin/bash



\# 测试Knative Pipeline

set -e



echo "=== 测试Knative Serverless Pipeline ==="



\# 获取ingestor服务URL

INGESTOR\_URL=$(kubectl get ksvc ingestor-service -n serverless-pipeline -o jsonpath='{.status.url}')

echo "Ingestor URL: $INGESTOR\_URL"



\# 等待服务就绪

echo "等待服务就绪..."

sleep 10



\# 发送测试事件

echo "发送测试事件..."

curl -X POST \\

&nbsp; -H "Ce-Id: test-001" \\

&nbsp; -H "Ce-Specversion: 1.0" \\

&nbsp; -H "Ce-Type: log.ingest" \\

&nbsp; -H "Ce-Source: test-client" \\

&nbsp; -H "Content-Type: application/json" \\

&nbsp; -d '{

&nbsp;   "log\_message": "2023-10-01T12:00:00Z - web-server - INFO - User login successful",

&nbsp;   "severity": "INFO",

&nbsp;   "service": "web-server",

&nbsp;   "timestamp": "2023-10-01T12:00:00Z"

&nbsp; }' \\

&nbsp; $INGESTOR\_URL



echo ""

echo "检查处理的事件:"

echo "kubectl logs -n serverless-pipeline deployment/loader-service -c user-container | tail -20"



echo ""

echo "=== 测试完成 ==="

4.3 scripts/cleanup.sh

bash

\#!/bin/bash



\# 清理部署

set -e



echo "=== 清理Knative Serverless Pipeline ==="



\# 删除所有资源

kubectl delete -f kubernetes/events.yaml --ignore-not-found

kubectl delete -f kubernetes/services.yaml --ignore-not-found

kubectl delete -f kubernetes/namespace.yaml --ignore-not-found



echo "=== 清理完成 ==="





5\. 简单的本地测试脚本

5.1 test\_local.py

python

\#!/usr/bin/env python3

"""

本地测试脚本 - 不需要Kubernetes

"""

import requests

import json

import time



def test\_pipeline\_locally():

&nbsp;   """本地测试完整管道"""

&nbsp;   

&nbsp;   # 启动所有服务（手动运行在不同终端）

&nbsp;   # python src/ingestor\_service.py

&nbsp;   # python src/parser\_service.py

&nbsp;   # python src/transformer\_service.py

&nbsp;   # python src/loader\_service.py

&nbsp;   

&nbsp;   test\_event = {

&nbsp;       "specversion": "1.0",

&nbsp;       "type": "log.ingest",

&nbsp;       "source": "test-client",

&nbsp;       "id": "test-event-001",

&nbsp;       "time": "2023-10-01T12:00:00Z",

&nbsp;       "datacontenttype": "application/json",

&nbsp;       "data": {

&nbsp;           "log\_message": "2023-10-01T12:00:00Z - web-server - INFO - User john@example.com login successful from 192.168.1.100",

&nbsp;           "severity": "INFO",

&nbsp;           "service": "web-server",

&nbsp;           "timestamp": "2023-10-01T12:00:00Z",

&nbsp;           "metadata": {

&nbsp;               "region": "us-east",

&nbsp;               "environment": "production"

&nbsp;           }

&nbsp;       }

&nbsp;   }

&nbsp;   

&nbsp;   # 模拟管道调用

&nbsp;   print("测试完整管道处理...")

&nbsp;   

&nbsp;   # 步骤1: Ingestor

&nbsp;   ingestor\_url = "http://localhost:8080"

&nbsp;   headers = {

&nbsp;       "Ce-Id": test\_event\["id"],

&nbsp;       "Ce-Specversion": test\_event\["specversion"],

&nbsp;       "Ce-Type": test\_event\["type"],

&nbsp;       "Ce-Source": test\_event\["source"],

&nbsp;       "Content-Type": test\_event\["datacontenttype"]

&nbsp;   }

&nbsp;   

&nbsp;   print("1. 发送到Ingestor...")

&nbsp;   response = requests.post(ingestor\_url, headers=headers, json=test\_event\["data"])

&nbsp;   print(f"  响应: {response.status\_code}")

&nbsp;   

&nbsp;   if response.status\_code == 200:

&nbsp;       ingestor\_result = response.json()

&nbsp;       print(f"  摄取ID: {ingestor\_result\['data']\['ingestion\_metadata']\['ingestion\_id']}")

&nbsp;       

&nbsp;       # 步骤2: Parser

&nbsp;       print("2. 发送到Parser...")

&nbsp;       parser\_url = "http://localhost:8081"

&nbsp;       response = requests.post(parser\_url, json=ingestor\_result)

&nbsp;       print(f"  响应: {response.status\_code}")

&nbsp;       

&nbsp;       if response.status\_code == 200:

&nbsp;           parser\_result = response.json()

&nbsp;           print(f"  使用的解析器: {parser\_result\['data']\['parsed\_data']\['parser\_used']}")

&nbsp;           

&nbsp;           # 步骤3: Transformer

&nbsp;           print("3. 发送到Transformer...")

&nbsp;           transformer\_url = "http://localhost:8082"

&nbsp;           response = requests.post(transformer\_url, json=parser\_result)

&nbsp;           print(f"  响应: {response.status\_code}")

&nbsp;           

&nbsp;           if response.status\_code == 200:

&nbsp;               transformer\_result = response.json()

&nbsp;               print(f"  事件哈希: {transformer\_result\['data']\['event\_hash']\[:8]}")

&nbsp;               

&nbsp;               # 步骤4: Loader

&nbsp;               print("4. 发送到Loader...")

&nbsp;               loader\_url = "http://localhost:8083"

&nbsp;               response = requests.post(loader\_url, json=transformer\_result)

&nbsp;               print(f"  响应: {response.status\_code}")

&nbsp;               

&nbsp;               if response.status\_code == 200:

&nbsp;                   loader\_result = response.json()

&nbsp;                   print(f"  存储结果: {loader\_result\['storage\_results']}")

&nbsp;                   print(f"  处理总数: {loader\_result\['metrics']\['total\_processed']}")

&nbsp;                   

&nbsp;                   print("\\n✅ 管道测试成功完成!")

&nbsp;                   return True

&nbsp;   

&nbsp;   print("\\n❌ 管道测试失败")

&nbsp;   return False



if \_\_name\_\_ == "\_\_main\_\_":

&nbsp;   print("启动本地测试...")

&nbsp;   print("请确保所有服务正在运行:")

&nbsp;   print("  ingestor:   python src/ingestor\_service.py")

&nbsp;   print("  parser:     python src/parser\_service.py (端口8081)")

&nbsp;   print("  transformer:python src/transformer\_service.py (端口8082)")

&nbsp;   print("  loader:     python src/loader\_service.py (端口8083)")

&nbsp;   print()

&nbsp;   

&nbsp;   input("按Enter键开始测试...")

&nbsp;   

&nbsp;   test\_pipeline\_locally()

5.2 run\_all\_services.py

python

\#!/usr/bin/env python3

"""

启动所有服务的简单脚本

"""

import subprocess

import sys

import time

import os



def run\_service(name, script, port):

&nbsp;   """运行一个服务"""

&nbsp;   env = os.environ.copy()

&nbsp;   env\['PORT'] = str(port)

&nbsp;   

&nbsp;   print(f"启动 {name} (端口 {port})...")

&nbsp;   return subprocess.Popen(

&nbsp;       \[sys.executable, script],

&nbsp;       env=env,

&nbsp;       stdout=subprocess.PIPE,

&nbsp;       stderr=subprocess.STDOUT,

&nbsp;       text=True

&nbsp;   )



def main():

&nbsp;   """启动所有服务"""

&nbsp;   services = \[

&nbsp;       ("ingestor", "src/ingestor\_service.py", 8080),

&nbsp;       ("parser", "src/parser\_service.py", 8081),

&nbsp;       ("transformer", "src/transformer\_service.py", 8082),

&nbsp;       ("loader", "src/loader\_service.py", 8083),

&nbsp;   ]

&nbsp;   

&nbsp;   processes = \[]

&nbsp;   

&nbsp;   try:

&nbsp;       for name, script, port in services:

&nbsp;           proc = run\_service(name, script, port)

&nbsp;           processes.append((name, proc))

&nbsp;           time.sleep(1)  # 给每个服务启动时间

&nbsp;       

&nbsp;       print("\\n所有服务已启动!")

&nbsp;       print("按 Ctrl+C 停止所有服务\\n")

&nbsp;       

&nbsp;       # 显示日志

&nbsp;       while True:

&nbsp;           for name, proc in processes:

&nbsp;               output = proc.stdout.readline()

&nbsp;               if output:

&nbsp;                   print(f"\[{name}] {output.strip()}")

&nbsp;           

&nbsp;   except KeyboardInterrupt:

&nbsp;       print("\\n正在停止服务...")

&nbsp;       for name, proc in processes:

&nbsp;           proc.terminate()

&nbsp;       print("服务已停止")



if \_\_name\_\_ == "\_\_main\_\_":

&nbsp;   main()

6\. README.md

markdown

\# Knative Serverless Pipeline



基于Knative构建的Serverless数据管道，用于处理和分析日志数据。



\## 架构

Event Generator → Ingestor → Parser → Transformer → Loader

↓ ↓ ↓ ↓

Broker ←→ Broker ←→ Broker ←→ Broker



text



\## 快速开始



\### 先决条件

\- Python 3.9+

\- Docker (可选)

\- Kubernetes集群 (可选)

\- Knative Serving \& Eventing (可选)



\### 本地运行

1\. 安装依赖:

&nbsp;  ```bash

&nbsp;  pip install -r requirements.txt

启动所有服务:



bash

python run\_all\_services.py

运行测试:



bash

python test\_local.py

使用Kubernetes部署

构建Docker镜像:



bash

docker build -t ingestor-service -f Dockerfile.ingestor .

docker build -t parser-service -f Dockerfile.parser .

docker build -t transformer-service -f Dockerfile.transformer .

docker build -t loader-service -f Dockerfile.loader .

部署到Kubernetes:



bash

./scripts/deploy.sh

运行测试:



bash

./scripts/test.sh

服务说明

1\. Event Generator

模拟日志事件生成器，用于测试。



2\. Ingestor Service

验证和丰富事件



添加摄取元数据



端口: 8080



3\. Parser Service

解析非结构化日志为结构化数据



支持多种日志格式



端口: 8081



4\. Transformer Service

数据转换和丰富



PII掩码



地理位置丰富



端口: 8082



5\. Loader Service

存储到多种后端(SQLite, JSON, CSV)



提供监控指标



端口: 8083



API端点

所有服务都提供:



POST / - 处理事件



GET /health - 健康检查



GET /metrics - 监控指标(仅Loader)



测试

bash

\# 本地测试

python test\_local.py



\# 负载测试

python -m src.event\_generator --rate 10 --duration 30



\# Kubernetes测试

./scripts/test.sh

清理

bash

\# 本地

按Ctrl+C停止所有服务



\# Kubernetes

./scripts/cleanup.sh

